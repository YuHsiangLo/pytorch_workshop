{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torchtext\n",
    "\n",
    "- What is it for?\n",
    "    - Raw data (csv, tsv) $\\rightarrow$ Datasets (preprocessed blocks of data)\n",
    "    - Datasets $\\rightarrow$ Iterators (handle numericalizing, batching, packaging, etc.)\n",
    "\n",
    "<img src=\"torchtext.png\">\n",
    "\n",
    "(image taken from [here](https://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw data (csv, tsv) $\\rightarrow$ Datasets\n",
    "\n",
    "- `Field`: how you want a certain column/field to be processed\n",
    "    - E.g., `\"I want to be tokenized\" -> [\"I\", \"want\", \"to\", \"be\", \"tokenize\"]`\n",
    "    - Documentation: you should totally read [it](https://torchtext.readthedocs.io/en/latest/data.html#field)!\n",
    "    - Common arguments:\n",
    "        - `sequential` (**True**): whether the datatype represents sequential data; **False** then no tokenization.\n",
    "        - `init_token` (**None**): a token that will be prepended to every example; **None** for no `init_token`.\n",
    "        - `eos_token` (**None**): a token that will be appended to every example; **None** for no `eos_token`.\n",
    "        - `lower` (**False**): whether to lowervase the text.\n",
    "        - `tokenize` (`string.split()`): the function used to tokenize strings.\n",
    "        - `pad_token` (**\"&lt;pad>\"**): padding token.\n",
    "        - `unk_token` (**\"&lt;unk>\"**): string token used to represent OOV words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field\n",
    "\n",
    "s = 'I want to be tokenized'\n",
    "\n",
    "# First some recap on lambda expression\n",
    "def tokenizer_explicit(x):\n",
    "    return x.split(sep=' ')\n",
    "\n",
    "# which is equivalent to a nameless function lambda x: x.split(sep=' ')\n",
    "\n",
    "WORD_1 = Field()\n",
    "print(WORD_1.preprocess(s))\n",
    "\n",
    "WORD_2 = Field()\n",
    "print(WORD_2.preprocess(s))\n",
    "\n",
    "WORD_3 = Field()\n",
    "print(WORD_3.preprocess(s))\n",
    "\n",
    "WORD_4 = Field()\n",
    "print(WORD_4.preprocess(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `NestedField`: a nested field holds another field (called nesting field) accepts an untokenized string.\n",
    "    - E.g., `\"I want to be nested-tokenized\" -> [[\"I\"], [\"w\", \"a\", \"n\", \"t\"], [\"t\", \"o\"]...]`\n",
    "    - [Documentation](https://torchtext.readthedocs.io/en/latest/data.html#nestedfield)\n",
    "    - Common arguments:\n",
    "        - `nesting_field` (**Field**): a field contained in this nested field.\n",
    "        - `tokenize` (`string.split()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import NestedField\n",
    "\n",
    "s = 'I want to be nested-tokenized'\n",
    "\n",
    "NESTING_FIELD = Field()\n",
    "WORD_5 = NestedField()\n",
    "print(WORD_5.preprocess(s))\n",
    "\n",
    "s = 'I_want_to_be_nested-tokenized'\n",
    "NESTING_FIELD = Field()\n",
    "WORD_6 = NestedField()\n",
    "print(WORD_6.preprocess(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn\n",
    "\n",
    "#### Q1\n",
    "```\n",
    "\"I want to be character-tokenized\" -> [\"i\", \"w\", \"a\", \"n\", \"t\", \"t\", \"o\"...]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'I want to be character-tokenized'\n",
    "\n",
    "WORD = Field()\n",
    "print(WORD.preprocess(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2\n",
    "\n",
    "```\n",
    "\"I want to be bi-gramized\" -> [(\"i\", \"want\"), (\"want\", \"to\"), (\"to\", \"be\")...]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'I want to be bi-gramized'\n",
    "\n",
    "BI_GRAM = Field()\n",
    "print(BI_GRAM.preprocess(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dataset: `TabularDataset`, `SequenceTaggingDataset`\n",
    "\n",
    "```\n",
    "1,\"Something wise.\",Someone sometime ->\n",
    "\n",
    "WORDS: [\"Something\", \"wise.\"]\n",
    "NAME: \"Someone sometime\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import TabularDataset\n",
    "\n",
    "tv_datafields = []\n",
    "\n",
    "trn, dev = TabularDataset\n",
    "\n",
    "tst_datafields = []\n",
    "\n",
    "tst = TabularDataset\n",
    "\n",
    "ex = next(iter(trn))\n",
    "print(ex.__dict__.keys())\n",
    "print(ex.WORDS)\n",
    "print(ex.NAME)\n",
    "\n",
    "ex = next(iter(tst))\n",
    "print(ex.__dict__.keys())\n",
    "print(ex.WORDS)\n",
    "print(ex.NAME)\n",
    "\n",
    "# Build vocab from training set\n",
    "print(WORD_2.vocab.stoi)\n",
    "print(WORD_3.vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets $\\rightarrow$ Iterators\n",
    "- `Iterator`: Defines an iterator that loads batches of data from a dataset.\n",
    "    - [Documentation](https://torchtext.readthedocs.io/en/latest/data.html#iterators)\n",
    "    - Common arguments:\n",
    "        - `dataset`\n",
    "        - `batch_size`\n",
    "        - `sort_key`: a key to use for sorting examples.\n",
    "        - `shuffle`: whether to shuffle examples between epochs.\n",
    "        - `sort`: whether to sort examples according to `sort_key`.\n",
    "        - `sort_within_batch`: whether to sort within each batch.\n",
    "        - `device` (**cpu**)\n",
    "\n",
    "- `BucketIterator`: Defines an iterator that batches examples of similar lengths together. Minimizes amount of padding needed while producing freshly shuffled batches for each new epoch.\n",
    "    - [Documentation](https://torchtext.readthedocs.io/en/latest/data.html#bucketiterator)\n",
    "    - Common arguments:\n",
    "        - `dataset`\n",
    "        - `batch_size`\n",
    "        - `sort_key`\n",
    "        - `shuffle`\n",
    "        - `sort`\n",
    "        - `device`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Iterator, BucketIterator\n",
    "\n",
    "trn_iter =\n",
    "\n",
    "dev_iter, tst_iter =\n",
    "\n",
    "ex = next(iter(trn_iter))\n",
    "print(ex)\n",
    "print('==========')\n",
    "print(ex.WORDS)\n",
    "print(ex.WORDS.size())  # (sequence_length, batch_size)\n",
    "print('----------')\n",
    "print(ex.NAME)\n",
    "print(ex.NAME.size())  # (batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn\n",
    "(thanks Miikka for the data and idea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/uralic.train', encoding='utf-8') as f:\n",
    "    n = 0\n",
    "    for line in f:\n",
    "        n += 1\n",
    "        print(line.strip())\n",
    "        if n == 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output format for each data point (line) should be\n",
    "```\n",
    "{WORD: 'todistuksensa',\n",
    " CHAR: ['t', 'o', 'd', 'i', 's', 't', 'u'...],\n",
    " LANG: 'fin'}\n",
    "```\n",
    "Also, for `WORD` and `CHAR` need to be prepend and append with `\"<start>\"` and `\"<end>\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = '<pad>'\n",
    "UNK = '<unk>'\n",
    "START = '<start>'\n",
    "END = '<end>'\n",
    "\n",
    "WORD = Field()\n",
    "\n",
    "CHAR = Field()\n",
    "\n",
    "LANG = Field()\n",
    "\n",
    "print(WORD.preprocess('afgaaninvinttikoiria'))\n",
    "print(CHAR.preprocess('afgaaninvinttikoiria'))\n",
    "print(LANG.preprocess('fin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafields = []\n",
    "              \n",
    "train, develop, test = TabularDataset\n",
    "\n",
    "ex = next(iter(train))\n",
    "print(ex.word)\n",
    "print(ex.char)\n",
    "print(ex.lang)\n",
    "\n",
    "# build the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = \n",
    "\n",
    "dev_iter, test_iter = \n",
    "\n",
    "ex = next(iter(train_iter))\n",
    "print(ex)\n",
    "print('==========')\n",
    "print(ex.word)\n",
    "print(ex.word.size())\n",
    "print('----------')\n",
    "print(ex.char)\n",
    "print(ex.char.size())\n",
    "print('----------')\n",
    "print(ex.lang)\n",
    "print(ex.lang.size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
