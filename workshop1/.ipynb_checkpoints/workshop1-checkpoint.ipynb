{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torchtext\n",
    "\n",
    "- What is it for?\n",
    "    - Raw data (csv, tsv) $\\rightarrow$ Datasets (preprocessed blocks of data)\n",
    "    - Datasets $\\rightarrow$ Iterators (handle numericalizing, batching, packaging, etc.)\n",
    "\n",
    "<img src=\"torchtext.png\">\n",
    "\n",
    "(image taken from [here](https://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw data (csv, tsv) $\\rightarrow$ Datasets\n",
    "\n",
    "- `Field`: how you want a certain column/field to be processed\n",
    "    - E.g., `\"I want to be tokenized\" -> [\"I\", \"want\", \"to\", \"be\", \"tokenize\"]`\n",
    "    - Documentation: you should totally read [it](https://torchtext.readthedocs.io/en/latest/data.html#field)!\n",
    "    - Common arguments:\n",
    "        - `sequential` (**True**): whether the datatype represents sequential data; **False** then no tokenization.\n",
    "        - `init_token` (**None**): a token that will be prepended to every example; **None** for no `init_token`.\n",
    "        - `eos_token` (**None**): a token that will be appended to every example; None for no `eos_token`.\n",
    "        - `lower` (**False**): whether to lowervase the text.\n",
    "        - `tokenize` (`string.split()`): the function used to tokenize strings.\n",
    "        - `pad_token` (**\"&lt;pad>\"**): padding token.\n",
    "        - `unk_token` (**\"&lt;unk>\"**): string token used to represent OOV words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'want', 'to', 'be', 'tokenized']\n",
      "['I', 'want', 'to', 'be', 'tokenized']\n",
      "I want to be tokenized\n",
      "['i', 'want', 'to', 'be', 'tokenized']\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import Field\n",
    "\n",
    "s = 'I want to be tokenized'\n",
    "\n",
    "# First some recap on lambda expression\n",
    "def tokenizer_explicit(x):\n",
    "    return x.split(sep=' ')\n",
    "\n",
    "# which is equivalent to a nameless function lambda x: x.split(sep=' ')\n",
    "\n",
    "WORD_1 = Field(tokenize=tokenizer_explicit)\n",
    "#WORD_1 = Field(tokenize=lambda x: x.split(sep=' '))\n",
    "#WORD_1 = Field(tokenize=str.split)\n",
    "print(WORD_1.preprocess(s))\n",
    "\n",
    "WORD_2 = Field(sequential=True)\n",
    "#WORD_2 = Field(sequential=True, init_token=\"<start>\", eos_token=\"<end>\")\n",
    "print(WORD_2.preprocess(s))\n",
    "\n",
    "WORD_3 = Field(sequential=False)\n",
    "print(WORD_3.preprocess(s))\n",
    "\n",
    "WORD_4 = Field(sequential=True, lower=True)\n",
    "print(WORD_4.preprocess(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `NestedField`: a nested field holds another field (called nesting field) accepts an untokenized string.\n",
    "    - E.g., `\"I want to be nested-tokenized\" -> [[\"I\"], [\"w\", \"a\", \"n\", \"t\"], [\"t\", \"o\"]...]`\n",
    "    - [Documentation](https://torchtext.readthedocs.io/en/latest/data.html#nestedfield)\n",
    "    - Common arguments:\n",
    "        - `nesting_field` (**Field**): a field contained in this nested field.\n",
    "        - `tokenize` (`string.split()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['I'], ['w', 'a', 'n', 't'], ['t', 'o'], ['b', 'e'], ['n', 'e', 's', 't', 'e', 'd', '-', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'e', 'd']]\n",
      "[['I'], ['w', 'a', 'n', 't'], ['t', 'o'], ['b', 'e'], ['n', 'e', 's', 't', 'e', 'd', '-', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'e', 'd']]\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import NestedField\n",
    "\n",
    "s = 'I want to be nested-tokenized'\n",
    "\n",
    "NESTING_FIELD = Field(tokenize=list)\n",
    "WORD_5 = NestedField(nesting_field=NESTING_FIELD)\n",
    "print(WORD_5.preprocess(s))\n",
    "\n",
    "s = 'I_want_to_be_nested-tokenized'\n",
    "NESTING_FIELD = Field(tokenize=list)\n",
    "WORD_6 = NestedField(nesting_field=NESTING_FIELD, tokenize=lambda x: x.split(sep='_'))\n",
    "print(WORD_6.preprocess(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn\n",
    "\n",
    "#### Q1\n",
    "```\n",
    "\"I want to be character-tokenized\" -> [\"i\", \"w\", \"a\", \"n\", \"t\", \"t\", \"o\"...]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'w', 'a', 'n', 't', 't', 'o', 'b', 'e', 'c', 'h', 'a', 'r', 'a', 'c', 't', 'e', 'r', '-', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'e', 'd']\n"
     ]
    }
   ],
   "source": [
    "s = 'I want to be character-tokenized'\n",
    "\n",
    "WORD = Field(tokenize=lambda x: [s for w in x.split(sep=' ') for s in w if s ], lower=True)\n",
    "print(WORD.preprocess(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2\n",
    "\n",
    "```\n",
    "\"I want to be bi-gramized\" -> [(\"i\", \"want\"), (\"want\", \"to\"), (\"to\", \"be\")...]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', 'want'), ('want', 'to'), ('to', 'be'), ('be', 'bi-gramized')]\n"
     ]
    }
   ],
   "source": [
    "s = 'I want to be bi-gramized'\n",
    "\n",
    "def tokenizer(s):\n",
    "    words = s.split(sep=' ')\n",
    "    return [(words[i].lower(), words[i+1].lower()) for i in range(len(words)-1)]\n",
    "\n",
    "BI_GRAM = Field(tokenize=tokenizer)\n",
    "print(BI_GRAM.preprocess(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dataset: `TabularDataset`, `SequenceTaggingDataset`\n",
    "\n",
    "```\n",
    "1,\"Something wise.\",Someone sometime ->\n",
    "\n",
    "WORDS: [\"Something\", \"wise\"]\n",
    "NAME: \"Someone sometime\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['WORDS', 'NAME'])\n",
      "['The', 'greatest', 'glory', 'in', 'living', 'lies', 'not', 'in', 'never', 'falling,', 'but', 'in', 'rising', 'every', 'time', 'we', 'fall.']\n",
      "Nelson Mandela\n",
      "dict_keys(['WORDS', 'NAME'])\n",
      "['You', 'miss', '100%', 'of', 'the', 'shots', 'you', \"don't\", 'take.']\n",
      "Wayne Gretzky\n",
      "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x129a60d90>>, {'<unk>': 0, '<pad>': 1, 'in': 2, 'you': 3, 'Always': 4, 'Just': 5, 'Let': 6, 'Life': 7, 'Spread': 8, 'The': 9, 'absolutely': 10, 'are': 11, 'busy': 12, 'but': 13, 'come': 14, 'else.': 15, 'ever': 16, 'every': 17, 'everyone': 18, 'everywhere': 19, 'fall.': 20, 'falling,': 21, 'glory': 22, 'go.': 23, 'greatest': 24, 'happens': 25, 'happier.': 26, 'is': 27, 'leaving': 28, 'lies': 29, 'like': 30, 'living': 31, 'love': 32, 'making': 33, 'never': 34, 'no': 35, 'not': 36, 'one': 37, 'other': 38, 'plans.': 39, 'remember': 40, 'rising': 41, 'that': 42, 'time': 43, 'to': 44, 'unique.': 45, 'we': 46, 'what': 47, 'when': 48, 'without': 49, \"you're\": 50})\n",
      "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x129a60d50>>, {'<unk>': 0, 'John Lennon': 1, 'Margaret Mead': 2, 'Mother Teresa': 3, 'Nelson Mandela': 4})\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import TabularDataset\n",
    "\n",
    "tv_datafields = [('NUM', None),\n",
    "                 ('WORDS', WORD_2),\n",
    "                 ('NAME', WORD_3)]\n",
    "\n",
    "trn, dev = TabularDataset.splits(\n",
    "    path='.',\n",
    "    train='train.csv',\n",
    "    validation='dev.csv',\n",
    "    format='csv',\n",
    "    skip_header=True,\n",
    "    fields=tv_datafields)\n",
    "\n",
    "tst_datafields = [('WORDS', WORD_2),\n",
    "                  ('NAME', WORD_3)]\n",
    "\n",
    "tst = TabularDataset(\n",
    "    path='./test.csv',\n",
    "    format='csv',\n",
    "    skip_header=False,\n",
    "    fields=tst_datafields)\n",
    "\n",
    "ex = next(iter(trn))\n",
    "print(ex.__dict__.keys())\n",
    "print(ex.WORDS)\n",
    "print(ex.NAME)\n",
    "\n",
    "ex = next(iter(tst))\n",
    "print(ex.__dict__.keys())\n",
    "print(ex.WORDS)\n",
    "print(ex.NAME)\n",
    "\n",
    "# Build vocab from training set\n",
    "WORD_2.build_vocab(trn)\n",
    "WORD_3.build_vocab(trn)\n",
    "print(WORD_2.vocab.stoi)\n",
    "print(WORD_3.vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets $\\rightarrow$ Iterators\n",
    "- `Iterator`: Defines an iterator that loads batches of data from a dataset.\n",
    "    - [Documentation](https://torchtext.readthedocs.io/en/latest/data.html#iterators)\n",
    "    - Common arguments:\n",
    "        - `dataset`\n",
    "        - `batch_size`\n",
    "        - `sort_key`: a key to use for sorting examples.\n",
    "        - `shuffle`: whether to shuffle examples between epochs.\n",
    "        - `sort`: whether to sort examples according to `sort_key`.\n",
    "        - `sort_within_batch`: whether to sort within each batch.\n",
    "        - `device` (**cpu**)\n",
    "\n",
    "- `BucketIterator`: Defines an iterator that batches examples of similar lengths together. Minimizes amount of padding needed while producing freshly shuffled batches for each new epoch.\n",
    "    - [Documentation](https://torchtext.readthedocs.io/en/latest/data.html#bucketiterator)\n",
    "    - Common arguments:\n",
    "        - `dataset`\n",
    "        - `batch_size`\n",
    "        - `sort_key`\n",
    "        - `shuffle`\n",
    "        - `sort`\n",
    "        - `device`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.data.batch.Batch of size 2]\n",
      "\t[.WORDS]:[torch.LongTensor of size 15x2]\n",
      "\t[.NAME]:[torch.LongTensor of size 2]\n",
      "==========\n",
      "tensor([[ 4,  8],\n",
      "        [40, 32],\n",
      "        [42, 19],\n",
      "        [ 3,  3],\n",
      "        [11, 23],\n",
      "        [10,  6],\n",
      "        [45, 35],\n",
      "        [ 5, 37],\n",
      "        [30, 16],\n",
      "        [18, 14],\n",
      "        [15, 44],\n",
      "        [ 1,  3],\n",
      "        [ 1, 49],\n",
      "        [ 1, 28],\n",
      "        [ 1, 26]])\n",
      "torch.Size([15, 2])\n",
      "----------\n",
      "tensor([2, 3])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import Iterator, BucketIterator\n",
    "\n",
    "trn_iter = BucketIterator(trn,\n",
    "                          batch_size=2,\n",
    "                          sort_key=len,\n",
    "                          shuffle=True)\n",
    "\n",
    "dev_iter, tst_iter = Iterator.splits((dev, tst),\n",
    "                                     batch_sizes=(1, 1),\n",
    "                                     sort=False,\n",
    "                                     shuffle=False)\n",
    "\n",
    "ex = next(iter(trn_iter))\n",
    "print(ex)\n",
    "print('==========')\n",
    "print(ex.WORDS)\n",
    "print(ex.WORDS.size())  # (sequence_length, batch_size)\n",
    "print('----------')\n",
    "print(ex.NAME)\n",
    "print(ex.NAME.size())  # (batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn\n",
    "(thanks Miikka for the data and idea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "сюлонь\tmyv\n",
      "вольпасьын\tkpv\n",
      "туялысьлы\tkpv\n",
      "курскоень\tmyv\n",
      "кӧнтусь\tkpv\n",
      "todistuksensa\tfin\n",
      "пойпанго\tmyv\n",
      "крезьгуръёсыз\tudm\n",
      "korrigeerimiseks\test\n",
      "чойяс\tkpv\n"
     ]
    }
   ],
   "source": [
    "with open('data/uralic.train', encoding='utf-8') as f:\n",
    "    n = 0\n",
    "    for line in f:\n",
    "        n += 1\n",
    "        print(line.strip())\n",
    "        if n == 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output format for each data point (line) should be\n",
    "```\n",
    "{WORD: 'todistuksensa',\n",
    " CHAR: ['t', 'o', 'd', 'i', 's', 't', 'u'...],\n",
    " LANG: 'fin'}\n",
    "```\n",
    "Also, for `WORD` and `CHAR` need to be prepend and append with `\"<start>\"` and `\"<end>\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "afgaaninvinttikoiria\n",
      "['a', 'f', 'g', 'a', 'a', 'n', 'i', 'n', 'v', 'i', 'n', 't', 't', 'i', 'k', 'o', 'i', 'r', 'i', 'a']\n",
      "fin\n"
     ]
    }
   ],
   "source": [
    "PAD = '<pad>'\n",
    "UNK = '<unk>'\n",
    "START = '<start>'\n",
    "END = '<end>'\n",
    "\n",
    "WORD = Field(sequential=False,\n",
    "             init_token=START,\n",
    "             eos_token=END,\n",
    "             pad_token=PAD,\n",
    "             unk_token=UNK)\n",
    "\n",
    "CHAR = Field(sequential=True,\n",
    "             tokenize=lambda s: [c for c in s],\n",
    "             lower=False,\n",
    "             init_token=START,\n",
    "             eos_token=END,\n",
    "             pad_token=PAD,\n",
    "             unk_token=UNK)\n",
    "\n",
    "LANG = Field(sequential=False)\n",
    "\n",
    "print(WORD.preprocess('afgaaninvinttikoiria'))\n",
    "print(CHAR.preprocess('afgaaninvinttikoiria'))\n",
    "print(LANG.preprocess('fin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "сюлонь\n",
      "['с', 'ю', 'л', 'о', 'н', 'ь']\n",
      "myv\n"
     ]
    }
   ],
   "source": [
    "datafields = [(('word', 'char'), (WORD, CHAR)),\n",
    "              ('lang', LANG)]\n",
    "              \n",
    "train, develop, test = TabularDataset.splits(\n",
    "    path='data',\n",
    "    train='uralic.train', validation=\"uralic.dev\", test='uralic.test',\n",
    "    format='tsv',\n",
    "    skip_header=False,\n",
    "    fields=datafields)\n",
    "\n",
    "ex = next(iter(train))\n",
    "print(ex.word)\n",
    "print(ex.char)\n",
    "print(ex.lang)\n",
    "\n",
    "WORD.build_vocab(train)\n",
    "CHAR.build_vocab(train)\n",
    "LANG.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.data.batch.Batch of size 5]\n",
      "\t[.lang]:[torch.LongTensor of size 5]\n",
      "\t[.word]:[torch.LongTensor of size 5]\n",
      "\t[.char]:[torch.LongTensor of size 13x5]\n",
      "==========\n",
      "tensor([ 308, 3586, 1715,  578, 5653])\n",
      "torch.Size([5])\n",
      "----------\n",
      "tensor([[ 2,  2,  2,  2,  2],\n",
      "        [11, 16,  9, 47, 35],\n",
      "        [20, 25, 23, 22, 31],\n",
      "        [ 6, 14, 47, 44,  7],\n",
      "        [47, 49, 23, 39, 25],\n",
      "        [ 6, 56, 41,  4,  7],\n",
      "        [39, 10, 23, 19,  3],\n",
      "        [ 3, 25,  9,  6,  1],\n",
      "        [ 1,  7,  3, 23,  1],\n",
      "        [ 1,  3,  1, 13,  1],\n",
      "        [ 1,  1,  1, 22,  1],\n",
      "        [ 1,  1,  1,  6,  1],\n",
      "        [ 1,  1,  1,  3,  1]])\n",
      "torch.Size([13, 5])\n",
      "----------\n",
      "tensor([1, 6, 1, 2, 4])\n",
      "torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "train_iter = BucketIterator(train,\n",
    "                            batch_size=5,\n",
    "                            sort_key=len,\n",
    "                            shuffle=True)\n",
    "\n",
    "dev_iter, test_iter = Iterator.splits((develop, test),\n",
    "                                       batch_sizes=(1, 1),\n",
    "                                       sort=False,\n",
    "                                       shuffle=True)\n",
    "\n",
    "ex = next(iter(train_iter))\n",
    "print(ex)\n",
    "print('==========')\n",
    "print(ex.word)\n",
    "print(ex.word.size())\n",
    "print('----------')\n",
    "print(ex.char)\n",
    "print(ex.char.size())\n",
    "print('----------')\n",
    "print(ex.lang)\n",
    "print(ex.lang.size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
